[{"feedAuthor":"","feedTitle":"xkcd.com","firstFetched":"2024-11-29T14:04:17.764361932Z","homepage":"https://xkcd.com/","link":"https://xkcd.com/3018/","published":"2024-11-29T05:00:00Z","summary":"","title":"Second Stage"},{"feedAuthor":"","feedTitle":"Schneier on Security","firstFetched":"2024-11-29T12:06:55.16373195Z","homepage":"https://www.schneier.com/","link":"https://www.schneier.com/blog/archives/2024/11/race-condition-attacks-against-llms.html","published":"2024-11-29T12:01:44Z","summary":"These are two attacks against the system components surrounding LLMs:\nWe propose that LLM Flowbreaking, following jailbreaking and prompt injection, joins as the third on the growing list of LLM attack types.  Flowbreaking is less about whether prompt or response guardrails can be bypassed, and more about whether user inputs and generated model outputs can adversely affect these other components in the broader implemented system. \n[â€¦]\nWhen confronted with a sensitive topic, Microsoft 365 Copilot and ChatGPT answer questions that their first-line guardrails are supposed to stop...","title":"Race Condition Attacks against LLMs"}]
